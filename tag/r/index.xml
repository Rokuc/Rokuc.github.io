<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Rok Jankovic</title>
    <link>https://rokuc.github.io/tag/r/</link>
      <atom:link href="https://rokuc.github.io/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 05 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://rokuc.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>R</title>
      <link>https://rokuc.github.io/tag/r/</link>
    </image>
    
    <item>
      <title>Simple coding with R</title>
      <link>https://rokuc.github.io/post/text_analysis_r/</link>
      <pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://rokuc.github.io/post/text_analysis_r/</guid>
      <description>&lt;h2 id=&#34;libraries-used-in-our-project&#34;&gt;Libraries Used in our project&lt;/h2&gt;
&lt;p&gt;In this section we will import a few packages that will be used through our analysis.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Importing ML packages
library(fastNaiveBayes)

# Text processing packages
library(tm)
library(textclean)
library(stringi)
library(stringr)
library(gmodels)

# Webscraping
library(rvest)

# Data manipulation packages
library(tidyverse)
library(data.table)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The machine learning package fastNaiveBayes, is optimal packages to work with large datasets it implements a fast version of the respective naïve Bayes algorithm. Next we implement some packages used for the preprocessing of text. The following rvest library helps scrape data from the web. Finally the tidyverse package is a popular package for data manipulation and the data.table package is useful to work with an alternative data structure.&lt;/p&gt;
&lt;h2 id=&#34;our-data&#34;&gt;Our data&lt;/h2&gt;
&lt;p&gt;In our project we will use data downloaded from the StockTwits platform. They provide pre-labeled microblogging data, which is perfect for our example, as we will not have to label data by ourselves. We will use a dataset of 1000 of which 500 are positive (bullish) labeled and 500 are labeled as negative (bearish).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;setwd(&amp;quot;E:/Projects&amp;quot;)
load(&amp;quot;sentiment_data.RData&amp;quot;)
head(sentiment_data,n=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First let us have a look at the first two documents to get a general idea of how our data set looks.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   Text
1    $AMC if you can’t beat em join just like $A  hedgies this thing so volatile I only need weeklys
2    $A  RECENT Twits Trends Today Change 18 % + https://t8sk.com/A
   Sentiment
1    Bullish
2    Bullish
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We have a dataset called sentiment_data with the column Text, and the column Sentiment. We can see that our data is very noisy, and will need lots of preprocessing. But before we move to our preprocessing section let us first download two more datasets we will need in our preprocessing stage. First we will download Ticker symbols from the finviz platform.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;us_stocks &amp;lt;- NULL
for(i in seq(1,7661,by = 20)){
  us_stocks_data &amp;lt;- read_html(paste0(&amp;quot;https://finviz.com/screener.ashx?v=111&amp;amp;r=&amp;quot;,i)) %&amp;gt;%
    html_nodes(&amp;quot;table&amp;quot;) %&amp;gt;%
    .[[17]] %&amp;gt;%
    html_table(header = TRUE)
  us_stocks &amp;lt;- rbind(us_stocks,us_stocks_data)
}
stocks &amp;lt;- us_stocks$Ticker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the section above we scrape data about all stocks that are listed on finviz, with the goal to obtain all the possible ticker names. This method could be applied for scraping a myriad of other things as well. Now let us move on to downloading emoji sentiment data, and creating a dictionary that can be used with the textclean package.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;emojis &amp;lt;- read_html(paste0(&amp;quot;http://kt.ijs.si/data/Emoji_sentiment_ranking/&amp;quot;)) %&amp;gt;%
  html_nodes(&amp;quot;#myTable&amp;quot;) %&amp;gt;%
  html_table(header = TRUE) %&amp;gt;%
  as.data.frame

emojis_lexicon &amp;lt;- data.frame(x=emojis$Char, y=emojis$Sentiment.score..1....1.)
emojis_lexicon$x &amp;lt;- iconv(emojis_lexicon$x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, &amp;quot;byte&amp;quot;)
emojis_lexicon$y &amp;lt;- ifelse(emojis_lexicon$y&amp;gt;=0,&amp;quot;pos_tag&amp;quot;,&amp;quot;neg_tag&amp;quot;)
emojis_lexicon &amp;lt;- data.table(emojis_lexicon)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we create a dictionary that we will later use in our preprocessing stage. We assign a positive score to emojis with a score greater or equal to 0 and a negative sentiment score to emojis with a negative score.&lt;/p&gt;
&lt;h2 id=&#34;pre-processing-stage&#34;&gt;Pre-processing stage&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sentiment_data &amp;lt;- data.frame(gsub(&amp;quot;http.+&amp;quot;,&amp;quot;linktag&amp;quot;,sentiment_data[,1]),sentiment_data[,2])
sentiment_data &amp;lt;- data.frame(stri_replace_all_regex(sentiment_data[,1],
                                                    pattern = str_c(&amp;quot;\\b(&amp;quot;, str_c(stocks, collapse=&amp;quot;|&amp;quot;), &amp;quot;)\\b&amp;quot;),
                                                    replacement = &amp;quot;cashtag&amp;quot;,
                                                    vectorize_all=FALSE),
                             sentiment_data[,2])
sentiment_data &amp;lt;- data.frame(replace_emoji(sentiment_data[,1], emoji_dt = emojis_lexicon),sentiment_data[,2])
sentiment_data &amp;lt;- data.frame(removePunctuation(sentiment_data[,1]),sentiment_data[,2])
sentiment_data &amp;lt;- data.frame(removeNumbers(sentiment_data[,1]),sentiment_data[,2])
sentiment_data &amp;lt;- data.frame(tolower(sentiment_data[,1]),sentiment_data[,2])

colnames(sentiment_data) &amp;lt;- c(&amp;quot;Text&amp;quot;, &amp;quot;Sentiment&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we use a variety of different approaches to manipulate our data. In the first line we use base r functions to replace urls starting with http with the word linktag. In the second line we replace all ticker symbols with the word cashtag, here we use the dataset stocks containing all the ticker symbols. Next we use our emoji_lexicon that we created in the previous section. Next we remove punctuation, numbers, and convert text to lowercase.&lt;/p&gt;
&lt;h2 id=&#34;final-steps-before-training-and-testing-our-algorithm&#34;&gt;Final steps before training and testing our algorithm&lt;/h2&gt;
&lt;p&gt;First let us create a dataset of 800 documents with 400 bullish documents and 400 bearish documents we will use to train our algorithms and a test set of 200 remaining documents.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;bull &amp;lt;- sentiment_data[sentiment_data$Sentiment==&amp;quot;Bullish&amp;quot;,]
bear &amp;lt;- sentiment_data[sentiment_data$Sentiment==&amp;quot;Bearish&amp;quot;,]

sentiment_data_train &amp;lt;- rbind(bull[1:400,],bear[1:400,])

sentiment_data_test &amp;lt;- rbind(bull[401:500,],bear[401:500,])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next let us create simple binary document term matrices.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;train_dtm &amp;lt;- DocumentTermMatrix(Corpus(VectorSource(sentiment_data_train$Text)))
test_dtm &amp;lt;- DocumentTermMatrix(Corpus(VectorSource(sentiment_data_test$Text)))

train_dtm$v[train_dtm$v &amp;gt; 0] &amp;lt;- 1
test_dtm$v[test_dtm$v &amp;gt; 0] &amp;lt;- 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finaly let us train and test our algorithm.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NB_classifier &amp;lt;- fnb.bernoulli(x=train_dtm, y=sentiment_data_train$Sentiment, sparse = TRUE)

NB_pred &amp;lt;- predict(NB_classifier, test_dtm, type = &amp;quot;class&amp;quot;)

CT&amp;lt;- CrossTable(NB_pred, sentiment_data$Sentiment,
                  prop.chisq = FALSE,
                  prop.t = FALSE,
                  dnn = c(&#39;predicted&#39;, &#39;actual&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us have a look at our output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;             | actual
   predicted |   Bearish |   Bullish | Row Total |
-------------|-----------|-----------|-----------|
     Bearish |        78 |        48 |       126 |
             |     0.619 |     0.381 |     0.630 |
             |     0.780 |     0.480 |           |
-------------|-----------|-----------|-----------|
     Bullish |        22 |        52 |        74 |
             |     0.297 |     0.703 |     0.370 |
             |     0.220 |     0.520 |           |
-------------|-----------|-----------|-----------|
Column Total |       100 |       100 |       200 |
             |     0.500 |     0.500 |           |
-------------|-----------|-----------|-----------|
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
